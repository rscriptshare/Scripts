{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf, isnan, when, count\n",
    "from pyspark.sql.types import DateType,ArrayType, DoubleType, FloatType\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "ss = SparkSession.builder.appName(\"spark_project\").getOrCreate()\n",
    "sqlContext = SQLContext(ss)\n",
    "df = ss.read.format('json').options(header='true').load('mini_sparkify_event_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_info(df):\n",
    "    '''\n",
    "    input:\n",
    "    df: dataframe\n",
    "    \n",
    "    output:\n",
    "    i)   number of rows and columns\n",
    "    ii)  top 3 rows of the dataframe\n",
    "    iii) data type\n",
    "    iv)  null value\n",
    "\n",
    "    '''\n",
    "    print('The dataframe has {} rows and {} columns'.format(df.count(), len(df.columns)))\n",
    "    check_null = df.toPandas()\n",
    "    display(df.limit(3).toPandas())\n",
    "    return df.printSchema(), (check_null.isnull().sum().sort_values(ascending = False))/df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe has 286500 rows and 18 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martha Tilston</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>277.89016</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538173362000</td>\n",
       "      <td>29</td>\n",
       "      <td>Rockpools</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352117000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Five Iron Frenzy</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>Long</td>\n",
       "      <td>236.09424</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538331630000</td>\n",
       "      <td>8</td>\n",
       "      <td>Canada</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352180000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adam Lambert</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>282.82730</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538173362000</td>\n",
       "      <td>29</td>\n",
       "      <td>Time For Miracles</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352394000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist       auth firstName gender  itemInSession lastName  \\\n",
       "0    Martha Tilston  Logged In     Colin      M             50  Freeman   \n",
       "1  Five Iron Frenzy  Logged In     Micah      M             79     Long   \n",
       "2      Adam Lambert  Logged In     Colin      M             51  Freeman   \n",
       "\n",
       "      length level                        location method      page  \\\n",
       "0  277.89016  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "1  236.09424  free  Boston-Cambridge-Newton, MA-NH    PUT  NextSong   \n",
       "2  282.82730  paid                 Bakersfield, CA    PUT  NextSong   \n",
       "\n",
       "    registration  sessionId               song  status             ts  \\\n",
       "0  1538173362000         29          Rockpools     200  1538352117000   \n",
       "1  1538331630000          8             Canada     200  1538352180000   \n",
       "2  1538173362000         29  Time For Miracles     200  1538352394000   \n",
       "\n",
       "                                           userAgent userId  \n",
       "0  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  \n",
       "1  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      9  \n",
       "2  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, artist           0.203812\n",
       " song             0.203812\n",
       " length           0.203812\n",
       " userAgent        0.029131\n",
       " firstName        0.029131\n",
       " gender           0.029131\n",
       " lastName         0.029131\n",
       " location         0.029131\n",
       " registration     0.029131\n",
       " method           0.000000\n",
       " page             0.000000\n",
       " level            0.000000\n",
       " sessionId        0.000000\n",
       " itemInSession    0.000000\n",
       " status           0.000000\n",
       " ts               0.000000\n",
       " auth             0.000000\n",
       " userId           0.000000\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>length</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>286500</td>\n",
       "      <td>228108</td>\n",
       "      <td>278154</td>\n",
       "      <td>286500</td>\n",
       "      <td>286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>114.41421291448516</td>\n",
       "      <td>249.1171819778458</td>\n",
       "      <td>1.5353588340844272E12</td>\n",
       "      <td>1041.526554973822</td>\n",
       "      <td>1.5409568898104834E12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>129.76726201140994</td>\n",
       "      <td>99.23517921058361</td>\n",
       "      <td>3.291321616327586E9</td>\n",
       "      <td>726.7762634630741</td>\n",
       "      <td>1.5075439608226302E9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "      <td>0.78322</td>\n",
       "      <td>1521380675000</td>\n",
       "      <td>1</td>\n",
       "      <td>1538352117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>1321</td>\n",
       "      <td>3024.66567</td>\n",
       "      <td>1543247354000</td>\n",
       "      <td>2474</td>\n",
       "      <td>1543799476000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary       itemInSession             length           registration  \\\n",
       "0   count              286500             228108                 278154   \n",
       "1    mean  114.41421291448516  249.1171819778458  1.5353588340844272E12   \n",
       "2  stddev  129.76726201140994  99.23517921058361    3.291321616327586E9   \n",
       "3     min                   0            0.78322          1521380675000   \n",
       "4     max                1321         3024.66567          1543247354000   \n",
       "\n",
       "           sessionId                     ts  \n",
       "0             286500                 286500  \n",
       "1  1041.526554973822  1.5409568898104834E12  \n",
       "2  726.7762634630741   1.5075439608226302E9  \n",
       "3                  1          1538352117000  \n",
       "4               2474          1543799476000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the statistics of non-string columns\n",
    "df.describe().toPandas().loc[:,['summary', 'itemInSession','length', 'registration', 'sessionId', 'ts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 226 userId, 17655 artist, 58480 song, 22 page, 1322 itemInSession, 114 location, 225 registration, 2354 sessionId, 277447 ts and 56 userAgent.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} userId, {} artist, {} song, {} page, {} itemInSession, {} location, {} registration, {} sessionId, {} ts and {} userAgent.'\\\n",
    "      .format(df.agg(F.countDistinct(df.userId)).collect()[0][0]\n",
    "              , df.agg(F.countDistinct(df.artist)).collect()[0][0]\n",
    "              , df.agg(F.countDistinct(df.song)).collect()[0][0]               \n",
    "              , df.agg(F.countDistinct(df.page)).collect()[0][0]\n",
    "              , df.agg(F.countDistinct(df.itemInSession)).collect()[0][0]             \n",
    "              , df.agg(F.countDistinct(df.location)).collect()[0][0]\n",
    "              , df.agg(F.countDistinct(df.registration)).collect()[0][0]\n",
    "              , df.agg(F.countDistinct(df.sessionId)).collect()[0][0]\n",
    "              , df.agg(F.countDistinct(df.ts)).collect()[0][0]\n",
    "              , df.agg(F.countDistinct(df.userAgent)).collect()[0][0]               \n",
    "             ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if userId has only 1 firstName & lastName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable('userId_Name')\n",
    "check_name_userId = ss.sql('''select distinct userId, concat(firstName, LastName) as Name from userId_Name''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_name_userId.write.mode(\"overwrite\").saveAsTable('userId_Name_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|userId|count(1)|\n",
      "+------+--------+\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_name_userId2 = ss.sql('''select userId, count(*) from userId_Name_2 group by userId having count(*) > 1''')\n",
    "check_name_userId2.show(3)\n",
    "#there's no result which mean each userId has only 1 Name, thus we can drop firstName & lastName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping some columns\n",
    "columns_to_drop = ['firstName', 'lastName', 'ts']\n",
    "df = df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martha Tilston</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>277.89016</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538173362000</td>\n",
       "      <td>29</td>\n",
       "      <td>Rockpools</td>\n",
       "      <td>200</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Five Iron Frenzy</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>236.09424</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1538331630000</td>\n",
       "      <td>8</td>\n",
       "      <td>Canada</td>\n",
       "      <td>200</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist       auth gender  itemInSession     length level  \\\n",
       "0    Martha Tilston  Logged In      M             50  277.89016  paid   \n",
       "1  Five Iron Frenzy  Logged In      M             79  236.09424  free   \n",
       "\n",
       "                         location method      page   registration  sessionId  \\\n",
       "0                 Bakersfield, CA    PUT  NextSong  1538173362000         29   \n",
       "1  Boston-Cambridge-Newton, MA-NH    PUT  NextSong  1538331630000          8   \n",
       "\n",
       "        song  status                                          userAgent userId  \n",
       "0  Rockpools     200  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...     30  \n",
       "1     Canada     200  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      9  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checker\n",
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check_null[check_null['page'] == 'Cancellation Confirmation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_null[check_null.userId.isin(check_null[check_null['page'] == 'Cancellation Confirmation']['userId'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      auth| count|\n",
      "+----------+------+\n",
      "| Logged In|278102|\n",
      "|Logged Out|  8249|\n",
      "|     Guest|    97|\n",
      "| Cancelled|    52|\n",
      "+----------+------+\n",
      "\n",
      "+------+------+\n",
      "|gender| count|\n",
      "+------+------+\n",
      "|     F|154578|\n",
      "|     M|123576|\n",
      "|  null|  8346|\n",
      "+------+------+\n",
      "\n",
      "+-----+------+\n",
      "|level| count|\n",
      "+-----+------+\n",
      "| paid|228162|\n",
      "| free| 58338|\n",
      "+-----+------+\n",
      "\n",
      "+------+------+\n",
      "|method| count|\n",
      "+------+------+\n",
      "|   PUT|261064|\n",
      "|   GET| 25436|\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|status| count|\n",
      "+------+------+\n",
      "|   200|259812|\n",
      "|   307| 26430|\n",
      "|   404|   258|\n",
      "+------+------+\n",
      "\n",
      "+--------------------+------+\n",
      "|                page| count|\n",
      "+--------------------+------+\n",
      "|            NextSong|228108|\n",
      "|                Home| 14457|\n",
      "|           Thumbs Up| 12551|\n",
      "|     Add to Playlist|  6526|\n",
      "|          Add Friend|  4277|\n",
      "|         Roll Advert|  3933|\n",
      "|               Login|  3241|\n",
      "|              Logout|  3226|\n",
      "|         Thumbs Down|  2546|\n",
      "|           Downgrade|  2055|\n",
      "|                Help|  1726|\n",
      "|            Settings|  1514|\n",
      "|               About|   924|\n",
      "|             Upgrade|   499|\n",
      "|       Save Settings|   310|\n",
      "|               Error|   258|\n",
      "|      Submit Upgrade|   159|\n",
      "|    Submit Downgrade|    63|\n",
      "|              Cancel|    52|\n",
      "|Cancellation Conf...|    52|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# value counts of some columns\n",
    "for column in ['auth', 'gender', 'level', 'method', 'status', 'page']:\n",
    "    df.groupBy(column).count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For those gender is null, fill as `unknown` <br> dropping status = 404 since it represents page not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286242, 15)\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna( { 'gender':'unknown'})#, 'b':0 } )\n",
    "df = df.filter(df.status != 404)\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                page|sum(Churn)|\n",
      "+--------------------+----------+\n",
      "|Cancellation Conf...|        52|\n",
      "|                Home|         0|\n",
      "|    Submit Downgrade|         0|\n",
      "+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define Churn, observe from above value counts, there should be 52 rows\n",
    "df = df.withColumn('Churn', F.when(F.col('page')=='Cancellation Confirmation', 1)\\\n",
    "                             .otherwise(0))\n",
    "df.groupBy('page').sum('Churn').orderBy('sum(Churn)', ascending=False).show(3)\n",
    "# df.toPandas().Churn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+-------------+------+-----+--------+------+----+------------+---------+----+------+---------+------+-----+\n",
      "|artist|auth|gender|itemInSession|length|level|location|method|page|registration|sessionId|song|status|userAgent|userId|Churn|\n",
      "+------+----+------+-------------+------+-----+--------+------+----+------------+---------+----+------+---------+------+-----+\n",
      "+------+----+------+-------------+------+-----+--------+------+----+------------+---------+----+------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check if there's any song without length\n",
    "df.filter((df.song.isNull()) & (df.length.isNotNull())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    page| count|\n",
      "+--------+------+\n",
      "|NextSong|228108|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check what the page is for row with value under column 'song'\n",
    "df.filter((df.song.isNotNull()) & (df.length.isNotNull())).groupBy('page').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if userId has only 1 gender & location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable('df_genderlocation')\n",
    "df_gender_location_check = ss.sql('''select distinct userId, gender, location from df_genderlocation ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+--------+\n",
      "|userId|gender|location|count(1)|\n",
      "+------+------+--------+--------+\n",
      "+------+------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gender_location_check.write.mode(\"overwrite\").saveAsTable('df_genderlocation2')\n",
    "df_gender_location_check2 = ss.sql('''\n",
    "select userId, gender, location, count(*) from df_genderlocation2 \n",
    "group by userId, gender, location having count(*) >1\n",
    "''')\n",
    "df_gender_location_check2.show()\n",
    "\n",
    "#each userId has just 1 gender and 1 location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation at userId level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df.withColumn('is_song', F.when(F.col('song').isNotNull(), 1).otherwise(0)) \\\n",
    ".withColumn('paid_level', F.when(F.col('level')=='paid', 1).otherwise(0)) \\\n",
    ".withColumn('free_level', F.when(F.col('level')=='free', 1).otherwise(0)) \\\n",
    ".withColumn('Submit_Downgrade_page', F.when(F.col('page')=='Submit Downgrade', 1).otherwise(0)) \\\n",
    ".withColumn('Downgrade', F.when(F.col('page')=='Downgrade', 1).otherwise(0)) \\\n",
    ".withColumn('Thumbs_Down', F.when(F.col('page')=='Thumbs Down', 1).otherwise(0)) \\\n",
    ".withColumn('Add_to_Playlist', F.when(F.col('page')=='Add to Playlist', 1).otherwise(0)) \\\n",
    ".withColumn('NextSong', F.when(F.col('page')=='NextSong', 1).otherwise(0)) \\\n",
    ".withColumn('Thumbs_Up', F.when(F.col('page')=='Thumbs Up', 1).otherwise(0)) \\\n",
    ".withColumn('Upgrade', F.when(F.col('page')=='Upgrade', 1).otherwise(0)) \\\n",
    ".withColumn('Add_Friend', F.when(F.col('page')=='Add Friend', 1).otherwise(0)) \\\n",
    ".withColumn('Help', F.when(F.col('page')=='Help', 1).otherwise(0)) \\\n",
    ".groupBy(['userId', 'gender', 'location'])\\\n",
    ".agg(\n",
    "#     F.max('InvoiceDate').alias('Last_Transaction_Date'),                                                                                         \n",
    "     F.sum('length').alias('sum_length_time'),\n",
    "     F.sum('is_song').alias('sum_song'), \n",
    "     F.countDistinct('artist').alias('Numofdifferentartist'), \n",
    "     F.countDistinct('song').alias('Numofdifferentsong'),                                            \n",
    "     F.sum('paid_level').alias('sum_paid_level'),  \n",
    "     F.sum('free_level').alias('sum_free_level'),  \n",
    "     F.sum('Submit_Downgrade_page').alias('sum_Submit_Downgrade_page'),  \n",
    "     F.sum('Downgrade').alias('sum_Downgrade'),  \n",
    "     F.sum('Thumbs_Down').alias('sum_Thumbs_Down'),  \n",
    "     F.sum('Add_to_Playlist').alias('sum_Add_to_Playlist'),  \n",
    "     F.sum('NextSong').alias('sum_NextSong'),  \n",
    "     F.sum('Thumbs_Up').alias('sum_Thumbs_Up'),  \n",
    "     F.sum('Upgrade').alias('sum_Upgrade'),\n",
    "     F.sum('Add_Friend').alias('sum_Add_Friend'),  \n",
    "     F.sum('Help').alias('sum_Help'),    \n",
    "     F.sum('Churn').alias('sum_Churn')\n",
    "    #  F.mean('RetailPrice').alias('MeanProductSpend'),\n",
    "  )\\\n",
    ".withColumn('Mean_length_time', F.col('sum_length_time') / F.col('sum_song'))\\\n",
    ".withColumn('Weight_1', F.when(F.col('sum_Churn')==1, 2.17).otherwise(0.65))\\\n",
    ".withColumn('Weight_2', F.when(F.col('sum_Churn')==1, 3).otherwise(1))\\\n",
    ".withColumn('Weight_3', F.when(F.col('sum_Churn')==1, 3.5).otherwise(1))\\\n",
    ".drop('sum_length_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226, 22)\n"
     ]
    }
   ],
   "source": [
    "# recap: There are 226 userId, 17655 artist, 58480 song, 22 page, 1322 itemInSession\n",
    "# , 114 location, 225 registration, 2354 sessionId, 277447 ts and 56 userAgent.\n",
    "# Just make sure we are getting 226 rows (or userId)\n",
    "\n",
    "print((df_group.count(), len(df_group.columns)))\n",
    "\n",
    "#It outputs the correct number of userid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    174\n",
       "1     52\n",
       "Name: sum_Churn, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_checker = df_group.toPandas()\n",
    "df_checker['sum_Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M          121\n",
       "F          104\n",
       "unknown      1\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_checker.gender.value_counts()\n",
    "\n",
    "# will drop the single unknown gender record later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of churn group vs. non-churn group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_churn = df_group.groupBy('sum_churn').agg(\n",
    "# .withColumn('is_song', F.when(F.col('song').isNotNull(), 1).otherwise(0)) \\\n",
    "# .withColumn('paid_level', F.when(F.col('level')=='paid', 1).otherwise(0)) \\\n",
    "# .withColumn('free_level', F.when(F.col('level')=='free', 1).otherwise(0)) \\\n",
    "# .withColumn('Submit_Downgrade_page', F.when(F.col('page')=='Submit Downgrade', 1).otherwise(0)) \\\n",
    "# .withColumn('Downgrade', F.when(F.col('page')=='Downgrade', 1).otherwise(0)) \\\n",
    "# .withColumn('Thumbs_Down', F.when(F.col('page')=='Thumbs Down', 1).otherwise(0)) \\\n",
    "# .withColumn('Add_to_Playlist', F.when(F.col('page')=='Add to Playlist', 1).otherwise(0)) \\\n",
    "# .withColumn('NextSong', F.when(F.col('page')=='NextSong', 1).otherwise(0)) \\\n",
    "# .withColumn('Thumbs_Up', F.when(F.col('page')=='Thumbs Up', 1).otherwise(0)) \\\n",
    "# .withColumn('Upgrade', F.when(F.col('page')=='Upgrade', 1).otherwise(0)) \\\n",
    "# .withColumn('Add_Friend', F.when(F.col('page')=='Add Friend', 1).otherwise(0)) \\\n",
    "# .withColumn('Help', F.when(F.col('page')=='Help', 1).otherwise(0)) \\\n",
    "\n",
    "\n",
    "#     F.max('InvoiceDate').alias('Last_Transaction_Date'),   \n",
    "     F.countDistinct('userId').alias('NumofuserId'),    \n",
    "#      F.sum('length').alias('sum_length_time'),\n",
    "     F.sum('sum_song').alias('sum_song2'), \n",
    "     F.countDistinct('Numofdifferentartist').alias('Numofdifferentartist2'), \n",
    "     F.countDistinct('Numofdifferentsong').alias('Numofdifferentsong2'),                                            \n",
    "     F.sum('sum_paid_level').alias('sum_paid_level2'),  \n",
    "     F.sum('sum_free_level').alias('sum_free_level2'),  \n",
    "     F.sum('sum_Submit_Downgrade_page').alias('sum_Submit_Downgrade_page2'),  \n",
    "     F.sum('sum_Downgrade').alias('sum_Downgrade2'),  \n",
    "     F.sum('sum_Thumbs_Down').alias('sum_Thumbs_Down2'),  \n",
    "     F.sum('sum_Add_to_Playlist').alias('sum_Add_to_Playlist2'),  \n",
    "     F.sum('sum_NextSong').alias('sum_NextSong2'),  \n",
    "     F.sum('sum_Thumbs_Up').alias('sum_Thumbs_Up2'),  \n",
    "     F.sum('sum_Upgrade').alias('sum_Upgrade2'),\n",
    "     F.sum('sum_Add_Friend').alias('sum_Add_Friend2'),  \n",
    "     F.sum('sum_Help').alias('sum_Help2'),    \n",
    "     F.sum('sum_Churn').alias('sum_Churn2')\n",
    "    #  F.mean('RetailPrice').alias('MeanProductSpend'),\n",
    "  )\\\n",
    ".withColumn('Mean_song', F.col('sum_song2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_differentartist', F.col('Numofdifferentartist2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_differentsong', F.col('Numofdifferentsong2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_paid_level', F.col('sum_paid_level2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_free_level', F.col('sum_free_level2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_Submit_Downgrade_page', F.col('sum_Submit_Downgrade_page2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_Downgrade', F.col('sum_Downgrade2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_Thumbs_Down', F.col('sum_Thumbs_Down2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_Add_to_Playlist', F.col('sum_Add_to_Playlist2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_NextSong', F.col('sum_NextSong2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_Thumbs_Up', F.col('sum_Thumbs_Up2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_Upgrade', F.col('sum_Upgrade2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_Add_Friend', F.col('sum_Add_Friend2') / F.col('NumofuserId'))\\\n",
    ".withColumn('Mean_Help', F.col('sum_Help2') / F.col('NumofuserId'))\\\n",
    ".drop('sum_paid_level2')\\\n",
    ".drop('sum_free_level2')\\\n",
    ".drop('sum_song2')\\\n",
    ".drop('Numofdifferentartist2')\\\n",
    ".drop('Numofdifferentsong2')\\\n",
    ".drop('sum_Submit_Downgrade_page2')\\\n",
    ".drop('sum_Downgrade2')\\\n",
    ".drop('sum_Thumbs_Down2')\\\n",
    ".drop('sum_Add_to_Playlist2')\\\n",
    ".drop('sum_NextSong2')\\\n",
    ".drop('sum_Thumbs_Up2')\\\n",
    ".drop('sum_Thumbs_Up2')\\\n",
    ".drop('sum_Upgrade2')\\\n",
    ".drop('sum_Help2')\\\n",
    ".drop('sum_Churn2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_churn</th>\n",
       "      <th>NumofuserId</th>\n",
       "      <th>sum_Add_Friend2</th>\n",
       "      <th>Mean_song</th>\n",
       "      <th>Mean_differentartist</th>\n",
       "      <th>Mean_differentsong</th>\n",
       "      <th>Mean_paid_level</th>\n",
       "      <th>Mean_free_level</th>\n",
       "      <th>Mean_Submit_Downgrade_page</th>\n",
       "      <th>Mean_Downgrade</th>\n",
       "      <th>Mean_Thumbs_Down</th>\n",
       "      <th>Mean_Add_to_Playlist</th>\n",
       "      <th>Mean_NextSong</th>\n",
       "      <th>Mean_Thumbs_Up</th>\n",
       "      <th>Mean_Upgrade</th>\n",
       "      <th>Mean_Add_Friend</th>\n",
       "      <th>Mean_Help</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>3641</td>\n",
       "      <td>1101.804598</td>\n",
       "      <td>0.971264</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>1123.591954</td>\n",
       "      <td>263.821839</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>9.873563</td>\n",
       "      <td>11.781609</td>\n",
       "      <td>31.540230</td>\n",
       "      <td>1101.804598</td>\n",
       "      <td>61.448276</td>\n",
       "      <td>2.224138</td>\n",
       "      <td>20.925287</td>\n",
       "      <td>8.545977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>636</td>\n",
       "      <td>699.884615</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>624.153846</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>6.480769</td>\n",
       "      <td>9.538462</td>\n",
       "      <td>19.961538</td>\n",
       "      <td>699.884615</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>2.153846</td>\n",
       "      <td>12.230769</td>\n",
       "      <td>4.596154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sum_churn  NumofuserId  sum_Add_Friend2    Mean_song  Mean_differentartist  \\\n",
       "0          0          174             3641  1101.804598              0.971264   \n",
       "1          1           52              636   699.884615              0.980769   \n",
       "\n",
       "   Mean_differentsong  Mean_paid_level  Mean_free_level  \\\n",
       "0            0.959770      1123.591954       263.821839   \n",
       "1            0.980769       624.153846       238.000000   \n",
       "\n",
       "   Mean_Submit_Downgrade_page  Mean_Downgrade  Mean_Thumbs_Down  \\\n",
       "0                    0.310345        9.873563         11.781609   \n",
       "1                    0.173077        6.480769          9.538462   \n",
       "\n",
       "   Mean_Add_to_Playlist  Mean_NextSong  Mean_Thumbs_Up  Mean_Upgrade  \\\n",
       "0             31.540230    1101.804598       61.448276      2.224138   \n",
       "1             19.961538     699.884615       35.750000      2.153846   \n",
       "\n",
       "   Mean_Add_Friend  Mean_Help  \n",
       "0        20.925287   8.545977  \n",
       "1        12.230769   4.596154  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_nonchurn_comparions = df_group_churn.toPandas()\n",
    "churn_nonchurn_comparions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sum_churn', 'NumofuserId', 'sum_Add_Friend2', 'Mean_song',\n",
       "       'Mean_differentartist', 'Mean_differentsong', 'Mean_paid_level',\n",
       "       'Mean_free_level', 'Mean_Submit_Downgrade_page', 'Mean_Downgrade',\n",
       "       'Mean_Thumbs_Down', 'Mean_Add_to_Playlist', 'Mean_NextSong',\n",
       "       'Mean_Thumbs_Up', 'Mean_Upgrade', 'Mean_Add_Friend', 'Mean_Help'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_nonchurn_comparions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_cols = ['userId']\n",
    "cat_cols = ['gender']\n",
    "non_cat_cols = [column for column in df_group.columns if column not in ('userId','gender','sum_Churn', 'location',\n",
    "                                                                       'Mean_length_time', 'Weight_1', 'Weight_2', 'Weight_3')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'userId, gender, sum_song, Numofdifferentartist, Numofdifferentsong, sum_paid_level, sum_free_level, sum_Submit_Downgrade_page, sum_Downgrade, sum_Thumbs_Down, sum_Add_to_Playlist, sum_NextSong, sum_Thumbs_Up, sum_Upgrade, sum_Add_Friend, sum_Help'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_string = ', '.join(ID_cols + cat_cols + non_cat_cols)\n",
    "column_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [\n",
    "    StringIndexer(\n",
    "        inputCol=col, \n",
    "        outputCol=col+'_Index') \n",
    "    for col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [\n",
    "    OneHotEncoder(\n",
    "        dropLast=False, \n",
    "        inputCol=indexer.getOutputCol(),\n",
    "        outputCol=indexer.getOutputCol()+'_Encoded') \n",
    "    for indexer in indexers]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_assembler = VectorAssembler(\n",
    "    inputCols=non_cat_cols, \n",
    "    outputCol='numFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(\n",
    "            inputCol='numFeatures',\n",
    "            outputCol='numFeatures_Scaled',\n",
    "            withStd=True,\n",
    "            withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders] + ['numFeatures_Scaled'], \n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group.write.mode(\"overwrite\").saveAsTable('user_add_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the required features \n",
    "df_features_query = ('''\n",
    "    SELECT {0}, sum_Churn, Weight_1, Weight_2, Weight_3\n",
    "    FROM user_add_features''' ).format(column_string)\n",
    "\n",
    "df_features = ss.sql(df_features_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226, 20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.count(), len(df_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping that 1 row with unknown gender\n",
    "df_features = df_features.filter( df_features.gender !='unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 20)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.count(), len(df_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Random Forest\n",
    "###### Starting with numTrees = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sum_Churn|count|\n",
      "+---------+-----+\n",
      "|        0|  119|\n",
      "|        1|   41|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sum_Churn|count|\n",
      "+---------+-----+\n",
      "|        0|   54|\n",
      "|        1|   11|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train test split: since it's an imbalanced dataset, we need to split the data by stratified sampling,\n",
    "# to make sure that each label has the same proportion in both train and test data\n",
    "train_data = df_features.sampleBy(\"sum_Churn\", fractions={0: 0.7, 1: 0.7}, seed=42)\n",
    "# subtract function: Return a new DataFrame containing rows in this DataFrame but not in another DataFrame.\n",
    "test_data = df_features.subtract(train_data)\n",
    "\n",
    "display(train_data.groupBy(\"sum_Churn\").count().show())\n",
    "test_data.groupBy(\"sum_Churn\").count().show()\n",
    "\n",
    "# train_data, test_data = df_features.randomSplit([0.7,0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol='sum_Churn',featuresCol='features', seed = 42, numTrees = 15)\n",
    "\n",
    "tmp_rfc = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp_rfc = [i for row in tmp_rfc for i in row]\n",
    "tmp_rfc += [scaler_assembler]\n",
    "tmp_rfc += [scaler]\n",
    "tmp_rfc += [assembler]\n",
    "tmp_rfc += [rfc]\n",
    "pl_rfc = Pipeline(stages=tmp_rfc)\n",
    "\n",
    "# fit\n",
    "rfc_model = pl_rfc.fit(train_data)\n",
    "#predict\n",
    "rfc_predictions = rfc_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_acc_rfc = rfc_predictions.select(['prediction','sum_Churn']).withColumn('sum_Churn', F.col('sum_Churn').cast(FloatType())).orderBy('prediction')\n",
    "eval_acc_rfc = eval_acc_rfc.select(['prediction','sum_Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_rfc = MulticlassMetrics(eval_acc_rfc.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 50.   4.]\n",
      " [ 10.   1.]]\n",
      "Accuracy = 0.7846153846153846\n"
     ]
    }
   ],
   "source": [
    "print(metrics_rfc.confusionMatrix().toArray())\n",
    "print(\"Accuracy = %s\" % metrics_rfc.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not_churn       0.83      0.93      0.88        54\n",
      "      churn       0.20      0.09      0.13        11\n",
      "\n",
      "avg / total       0.73      0.78      0.75        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['not_churn', 'churn']\n",
    "print(classification_report(eval_acc_rfc.toPandas().sum_Churn, eval_acc_rfc.toPandas().prediction, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try tuning the parameter of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsamplingRate = 0.75 , numTrees = 20 , maxDepth = 3\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n",
      "subsamplingRate = 0.75 , numTrees = 20 , maxDepth = 4\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n",
      "subsamplingRate = 0.75 , numTrees = 26 , maxDepth = 3\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n",
      "subsamplingRate = 0.75 , numTrees = 26 , maxDepth = 4\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n",
      "subsamplingRate = 0.75 , numTrees = 32 , maxDepth = 3\n",
      "[[ 54.   0.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8307692307692308\n",
      "==========================================\n",
      "subsamplingRate = 0.75 , numTrees = 32 , maxDepth = 4\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n",
      "subsamplingRate = 0.75 , numTrees = 38 , maxDepth = 3\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n",
      "subsamplingRate = 0.75 , numTrees = 38 , maxDepth = 4\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n",
      "subsamplingRate = 1 , numTrees = 20 , maxDepth = 3\n",
      "[[ 54.   0.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8307692307692308\n",
      "==========================================\n",
      "subsamplingRate = 1 , numTrees = 20 , maxDepth = 4\n",
      "[[ 52.   2.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8\n",
      "==========================================\n",
      "subsamplingRate = 1 , numTrees = 26 , maxDepth = 3\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n",
      "subsamplingRate = 1 , numTrees = 26 , maxDepth = 4\n",
      "[[ 54.   0.]\n",
      " [ 10.   1.]]\n",
      "Accuracy = 0.8461538461538461\n",
      "==========================================\n",
      "subsamplingRate = 1 , numTrees = 32 , maxDepth = 3\n",
      "[[ 54.   0.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8307692307692308\n",
      "==========================================\n",
      "subsamplingRate = 1 , numTrees = 32 , maxDepth = 4\n",
      "[[ 54.   0.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8307692307692308\n",
      "==========================================\n",
      "subsamplingRate = 1 , numTrees = 38 , maxDepth = 3\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n",
      "subsamplingRate = 1 , numTrees = 38 , maxDepth = 4\n",
      "[[ 53.   1.]\n",
      " [ 11.   0.]]\n",
      "Accuracy = 0.8153846153846154\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "for rate in (0.75, 1):\n",
    "    for tree in range(20,40,6):\n",
    "        for depth in range(3,5,1):\n",
    "            print('subsamplingRate = {} , numTrees = {} , maxDepth = {}'.format(rate,tree,depth))\n",
    "            rfc_tune = RandomForestClassifier(labelCol='sum_Churn',featuresCol='features', seed = 42, subsamplingRate = rate, numTrees = tree, maxDepth = depth )\n",
    "\n",
    "            tmp_rfc_tune = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "            tmp_rfc_tune = [i for row in tmp_rfc_tune for i in row]\n",
    "            tmp_rfc_tune += [scaler_assembler]\n",
    "            tmp_rfc_tune += [scaler]\n",
    "            tmp_rfc_tune += [assembler]\n",
    "            tmp_rfc_tune += [rfc_tune]\n",
    "\n",
    "            pl_rfc_tune = Pipeline(stages=tmp_rfc_tune)\n",
    "            rfc_model_tune = pl_rfc_tune.fit(train_data)\n",
    "            rfc_predictions_tune = rfc_model_tune.transform(test_data)\n",
    "\n",
    "            eval_acc_rfc_tune = rfc_predictions_tune.select(['prediction','sum_Churn']).withColumn('sum_Churn', F.col('sum_Churn').cast(FloatType())).orderBy('prediction')\n",
    "            eval_acc_rfc_tune = eval_acc_rfc_tune.select(['prediction','sum_Churn'])\n",
    "            metrics_rfc_tune = MulticlassMetrics(eval_acc_rfc_tune.rdd.map(tuple))\n",
    "            print(metrics_rfc_tune.confusionMatrix().toArray())\n",
    "            print(\"Accuracy = %s\" % metrics_rfc_tune.accuracy)\n",
    "            print('==========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the parameters we tried, subsamplingRate = 1, numTrees = 26, maxDepth 4 output the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 54.   0.]\n",
      " [ 10.   1.]]\n",
      "Accuracy = 0.8461538461538461\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not_churn       0.84      1.00      0.92        54\n",
      "      churn       1.00      0.09      0.17        11\n",
      "\n",
      "avg / total       0.87      0.85      0.79        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_tune = RandomForestClassifier(labelCol='sum_Churn',featuresCol='features', seed = 42, subsamplingRate = 1, numTrees = 26, maxDepth = 4 )\n",
    "\n",
    "tmp_rfc_tune = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp_rfc_tune = [i for row in tmp_rfc_tune for i in row]\n",
    "tmp_rfc_tune += [scaler_assembler]\n",
    "tmp_rfc_tune += [scaler]\n",
    "tmp_rfc_tune += [assembler]\n",
    "tmp_rfc_tune += [rfc_tune]\n",
    "\n",
    "pl_rfc_tune = Pipeline(stages=tmp_rfc_tune)\n",
    "rfc_model_tune = pl_rfc_tune.fit(train_data)\n",
    "rfc_predictions_tune = rfc_model_tune.transform(test_data)\n",
    "\n",
    "eval_acc_rfc_tune = rfc_predictions_tune.select(['prediction','sum_Churn']).withColumn('sum_Churn', F.col('sum_Churn').cast(FloatType())).orderBy('prediction')\n",
    "eval_acc_rfc_tune = eval_acc_rfc_tune.select(['prediction','sum_Churn'])\n",
    "metrics_rfc_tune = MulticlassMetrics(eval_acc_rfc_tune.rdd.map(tuple))\n",
    "print(metrics_rfc_tune.confusionMatrix().toArray())\n",
    "print(\"Accuracy = %s\" % metrics_rfc_tune.accuracy)\n",
    "print(classification_report(eval_acc_rfc_tune.toPandas().sum_Churn, eval_acc_rfc_tune.toPandas().prediction, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try another supervised learning model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol='features',labelCol='sum_Churn')\n",
    "\n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for row in tmp for i in row]\n",
    "tmp += [scaler_assembler]\n",
    "tmp += [scaler]\n",
    "tmp += [assembler]\n",
    "tmp += [log_reg]\n",
    "\n",
    "pl = Pipeline(stages=tmp)\n",
    "\n",
    "fit_model = pl.fit(train_data)\n",
    "\n",
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 51.   3.]\n",
      " [  9.   2.]]\n",
      "Accuracy = 0.8153846153846154\n"
     ]
    }
   ],
   "source": [
    "eval_acc = results.select(['prediction','sum_Churn']).withColumn('sum_Churn', F.col('sum_Churn').cast(FloatType())).orderBy('prediction')\n",
    "eval_acc = eval_acc.select(['prediction','sum_Churn'])\n",
    "\n",
    "metrics = MulticlassMetrics(eval_acc.rdd.map(tuple))\n",
    "\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(\"Accuracy = %s\" % metrics.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not_churn       0.85      0.94      0.89        54\n",
      "      churn       0.40      0.18      0.25        11\n",
      "\n",
      "avg / total       0.77      0.82      0.79        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(eval_acc.toPandas().sum_Churn, eval_acc.toPandas().prediction, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since it's an imbalanced dataset with churn = 52 & non-churn = 173, let's place more weight on churn and see the change of accuracy and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weightCol = Weight_1 \n",
      "\n",
      "[[ 28.  26.]\n",
      " [  2.   9.]]\n",
      "\n",
      "Accuracy = 0.5692307692307692\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not_churn       0.93      0.52      0.67        54\n",
      "      churn       0.26      0.82      0.39        11\n",
      "\n",
      "avg / total       0.82      0.57      0.62        65\n",
      "\n",
      "=============================================================================================\n",
      "weightCol = Weight_2 \n",
      "\n",
      "[[ 33.  21.]\n",
      " [  2.   9.]]\n",
      "\n",
      "Accuracy = 0.6461538461538462\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not_churn       0.94      0.61      0.74        54\n",
      "      churn       0.30      0.82      0.44        11\n",
      "\n",
      "avg / total       0.83      0.65      0.69        65\n",
      "\n",
      "=============================================================================================\n",
      "weightCol = Weight_3 \n",
      "\n",
      "[[ 28.  26.]\n",
      " [  2.   9.]]\n",
      "\n",
      "Accuracy = 0.5692307692307692\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  not_churn       0.93      0.52      0.67        54\n",
      "      churn       0.26      0.82      0.39        11\n",
      "\n",
      "avg / total       0.82      0.57      0.62        65\n",
      "\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "for column in ['Weight_1','Weight_2','Weight_3']:\n",
    "    log_reg = LogisticRegression(featuresCol='features',labelCol='sum_Churn', weightCol=column)\n",
    "\n",
    "    tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "    tmp = [i for row in tmp for i in row]\n",
    "    tmp += [scaler_assembler]\n",
    "    tmp += [scaler]\n",
    "    tmp += [assembler]\n",
    "    tmp+=[log_reg]\n",
    "\n",
    "    pl = Pipeline(stages=tmp)\n",
    "\n",
    "    fit_model = pl.fit(train_data)\n",
    "\n",
    "    results = fit_model.transform(test_data)\n",
    "    eval_acc = results.select(['prediction','sum_Churn']).withColumn('sum_Churn', F.col('sum_Churn').cast(FloatType())).orderBy('prediction')\n",
    "    eval_acc = eval_acc.select(['prediction','sum_Churn'])\n",
    "\n",
    "    metrics = MulticlassMetrics(eval_acc.rdd.map(tuple))\n",
    "    print('weightCol = {} \\n'.format(column))\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "    print(\"\\nAccuracy = %s\" % metrics.accuracy)\n",
    "    print(classification_report(eval_acc.toPandas().sum_Churn, eval_acc.toPandas().prediction, target_names=target_names))\n",
    "    print('=============================================================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
